import os
import tempfile

import joblib as jl
import matplotlib.pyplot as plt  # type: ignore
import mlflow  # type: ignore
import numpy as np  # type: ignore
import pandas as pd  # type: ignore
import prolcs.search.operators.drugowitsch as dop
from deap import creator, tools  # type: ignore
from prolcs.common import initRepeat_binom
from prolcs.literal.hyperparams import HParams
from prolcs.literal.model import Model
from prolcs.search.ga.drugowitsch import GADrugowitsch
from sklearn import metrics  # type: ignore
from sklearn.preprocessing import StandardScaler
from sklearn.utils import check_random_state  # type: ignore

np.seterr(all="warn")


class Toolbox(dop.Toolbox):
    def __init__(self, matchcls, gaparams, random_state):
        """
        A toolbox based on ``prolcs.search.operators.drugowitsch.Toolbox`` with
        initialization and evaluation as defined in (Drugowitsch, 2007).
        """
        super().__init__(tournsize=gaparams["tournsize"])

        random_state = check_random_state(random_state)

        # “An individual is initially generated by randomly choosing the number
        # of classifiers it represents and then initializing the matching
        # function of each of its classifiers, again randomly.”
        self.register("gene", matchcls.random, random_state=random_state)
        self.register("genotype",
                      initRepeat_binom,
                      creator.Genotype,
                      self.gene,
                      n=gaparams["n"],
                      p=gaparams["p"],
                      random_state=random_state)

        self.register("population", tools.initRepeat, list, self.genotype)

        def _evaluate(genotype, X, y):
            genotype.phenotype = Model(matchs=genotype,
                                       random_state=random_state).fit(X, y)
            return (genotype.phenotype.p_M_D_, )

        self.register("evaluate", _evaluate)


def log_array(a, label):
    f = tempfile.NamedTemporaryFile(prefix=f"{label}-", suffix=f".csv")
    pd.DataFrame(a).to_csv(f.name)
    mlflow.log_artifact(f.name)
    f.close()


def experiment(name,
               matchcls,
               gaparams,
               X,
               y,
               X_test,
               y_test_true,
               X_denoised,
               y_denoised,
               n_iter,
               seed,
               show,
               sample_size,
               standardize=False):
    mlflow.set_experiment(name)
    with mlflow.start_run() as run:
        mlflow.log_params(HParams().__dict__)
        mlflow.log_param("seed", seed)
        mlflow.log_param("train.size", sample_size)

        log_array(X, "X")
        log_array(y, "y")
        log_array(X_test, "X_test")
        log_array(y_test_true, "y_test_true")
        log_array(X_denoised, "X_denoised")
        log_array(y_denoised, "y_denoised")

        if standardize:
            scaler_X = StandardScaler()
            scaler_y = StandardScaler()
            X = scaler_X.fit_transform(X)
            X_test = scaler_X.transform(X_test)
            y = scaler_y.fit_transform(y)
            y_test_true = scaler_y.transform(y_test_true)

        random_state = check_random_state(seed)

        estimator = GADrugowitsch(Toolbox(matchcls, gaparams, random_state=random_state),
                                  n_iter=n_iter,
                                  random_state=random_state)
        estimator = estimator.fit(X, y)

        # make predictions for test data
        y_test, var = estimator.predict_mean_var(X_test)

        # get unmixed classifier predictions
        y_cls = estimator.predicts(X)

        if standardize:
            X = scaler_X.inverse_transform(X)
            X_test = scaler_X.inverse_transform(X_test)
            y = scaler_y.inverse_transform(y)
            y_test = scaler_y.inverse_transform(y_test)
            var = scaler_y.scale_**2 * var
            y_cls = scaler_y.inverse_transform(y_cls)

        log_array(y_test, "y_test")
        log_array(var, "var")
        log_array(np.hstack(y_cls), "y_cls")

        # two additional statistics to maybe better gauge solution performance
        mse = metrics.mean_squared_error(y_test_true, y_test)
        r2 = metrics.r2_score(y_test_true, y_test)
        mlflow.log_metric("elitist.size", estimator.size_[0], n_iter)
        mlflow.log_metric("elitist.p_M_D", estimator.p_M_D_[0], n_iter)
        mlflow.log_metric("elitist.mse", mse, n_iter)
        mlflow.log_metric("elitist.r2-score", r2, n_iter)

        # store the model, you never know when you need it
        model_file = f"models/Model {seed}.joblib"
        jl.dump(estimator.frozen(), model_file)
        mlflow.log_artifact(model_file)

        fig, ax = plot_prediction(X=X,
                                  y=y,
                                  X_test=X_test,
                                  y_test=y_test,
                                  var=var,
                                  X_denoised=X_denoised,
                                  y_denoised=y_denoised)

        plot_cls(X=X, y=y_cls, ax=ax)
        add_title(ax, estimator.size_[0], estimator.p_M_D_[0], mse, r2)
        save_plot(fig, seed)

        if show:
            plt.show()


def plot_prediction(X,
                    y,
                    X_test,
                    y_test,
                    var,
                    X_denoised=None,
                    y_denoised=None):
    fig, ax = plt.subplots()

    # plot input data
    ax.plot(X.ravel(), y.ravel(), "r+")

    if X_denoised is not None and y_denoised is not None:
        # plot denoised input data for visual reference
        ax.plot(X_denoised.ravel(), y_denoised.ravel(), "k--")

    # plot test data
    X_test_ = X_test.ravel()
    perm = np.argsort(X_test_)
    X_test_ = X_test_[perm]
    y_test_ = y_test.ravel()[perm]
    var_ = var.ravel()[perm]
    std = np.sqrt(var_)
    ax.plot(X_test_, y_test_, "b-")
    ax.plot(X_test_, y_test_ - std, "b--", linewidth=0.5)
    ax.plot(X_test_, y_test_ + std, "b--", linewidth=0.5)
    ax.fill_between(X_test_, y_test_ - std, y_test_ + std, alpha=0.2)

    return fig, ax


def plot_cls(X, y, ax=None):
    """
    Parameters
    ----------
    X : array of shape (N, 1)
        Points for which the classifiers made predictions.
    y : array of shape (K, N, Dy)
        Predictions of the ``K`` classifiers.
    """
    if ax is None:
        fig, ax = plt.subplots()
    else:
        fig = None

    for k in range(len(y)):
        ax.plot(X.ravel(),
                y[k],
                c="grey",
                linestyle="-",
                linewidth=0.5,
                alpha=0.7,
                zorder=10)

    return fig, ax


def add_title(ax, K, p_M_D, mse, r2):
    # add metadata to plot for ease of use
    ax.set(title=(f"K = {K}, "
                  f"p(M|D) = {(p_M_D):.2}, "
                  f"mse = {mse:.2}, "
                  f"r2 = {r2:.2}"))


def save_plot(fig, seed):
    # store the figure (e.g. so we can run headless)
    fig_folder = "latest-final-approximations"
    if not os.path.exists(fig_folder):
        os.makedirs(fig_folder)
    fig_file = f"{fig_folder}/Final approximation {seed}.pdf"
    print(f"Storing final approximation figure in {fig_file}")
    fig.savefig(fig_file)
    mlflow.log_artifact(fig_file)
